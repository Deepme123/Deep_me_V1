from typing import Optional, List, Generator
from openai import OpenAI
from app.models.emotion import EmotionSession, EmotionStep
from app.core.prompt_loader import get_system_prompt

LLM_MODEL = "gpt-3.5-turbo"
LLM_TEMPERATURE = 0.7
LLM_MAX_TOKENS = 800

client = OpenAI()

# ... _condense_history() ë™ì¼ ...

def _condense_history(history: list[str], max_chars: int = 1000) -> str:
    """
    ëŒ€í™” ê¸°ë¡(history)ì„ ìµœëŒ€ max_chars ê¸¸ì´ì— ë§ì¶° ì¶•ì•½í•¨.
    
    Args:
        history (list[str]): ì´ì „ ëŒ€í™”ë“¤ì˜ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸.
        max_chars (int): ìµœëŒ€ ê¸¸ì´ ì œí•œ. ê¸°ë³¸ê°’ì€ 1000ì.

    Returns:
        str: ìµœê·¼ ëŒ€í™” ì¤‘ì‹¬ìœ¼ë¡œ ì¶•ì•½ëœ ë¬¸ìì—´.
    """
    combined = "\n".join(history).strip()

    # ì´ ê¸¸ì´ê°€ ì œí•œë³´ë‹¤ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if len(combined) <= max_chars:
        return combined

    # ê¸¸ë©´ ìµœê·¼ ë‚´ìš© ì¤‘ì‹¬ìœ¼ë¡œ ì˜ë¼ì„œ ë°˜í™˜
    return "...\n" + combined[-max_chars:]


def _build_messages(
    user_input: str,
    emotion_label: Optional[str],
    topic: Optional[str],
    history_snippet: str,
    system_prompt: Optional[str] = None,
):
    sys = system_prompt or get_system_prompt()
    ctx_parts: list[str] = []
    if emotion_label:
        ctx_parts.append(f"ê°ì •: {emotion_label}")
    if topic:
        ctx_parts.append(f"ì£¼ì œ: {topic}")
    if history_snippet:
        ctx_parts.append(f"ìµœê·¼ ëŒ€í™”:\n{history_snippet}")

    context_block = "\n".join(ctx_parts).strip()

    messages = [{"role": "system", "content": sys}]
    if context_block:
        messages.append({"role": "user", "content": context_block})
    messages.append({"role": "user", "content": user_input})
    return messages


def generate_noa_response(
    user_input: str,
    session: EmotionSession,
    recent_steps: List[EmotionStep],
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    system_prompt: Optional[str] = None,
) -> str:
    messages = _build_messages(
        user_input,
        session.emotion_label,
        session.topic,
        _condense_history([  # âœ… ì—¬ê¸°!
            f"ìœ ì €: {s.user_input}\nGPT: {s.gpt_response}" for s in recent_steps
        ]),
        system_prompt,
    )
    resp = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
        temperature=temperature or LLM_TEMPERATURE,
        max_tokens=max_tokens or LLM_MAX_TOKENS,
    )
    print("ğŸ” ë©”ì‹œì§€:", messages)
    return resp.choices[0].message.content.strip()



def stream_noa_response(
    *,
    user_input: str,
    session: EmotionSession,
    recent_steps: List[EmotionStep],
    system_prompt: Optional[str] = None,
    temperature: float = LLM_TEMPERATURE,
    max_tokens: int = 400,
) -> Generator[str, None, None]:
    """í† í° ë‹¨ìœ„ GPT ì‘ë‹µì„ yield"""
    messages = _build_messages(
        user_input,
        session.emotion_label,
        session.topic,
        _condense_history([  # âœ… ì—¬ê¸°ë§Œ ê³ ì¹¨
            f"ìœ ì €: {s.user_input}\nGPT: {s.gpt_response}" for s in recent_steps
        ]),
        system_prompt,
    )
    stream = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        stream=True,
    )
    collected = []
    for chunk in stream:
        delta = chunk.choices[0].delta.content or ""
        if delta:
            collected.append(delta)
            yield delta
    print("ğŸ” ë©”ì‹œì§€:", messages)