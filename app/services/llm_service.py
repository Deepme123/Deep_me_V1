# app/services/llm_service.py
from __future__ import annotations

import os
import logging
from typing import Iterable, Generator, Optional
from openai import OpenAI, BadRequestError

logger = logging.getLogger(__name__)

# ===== Config =====
MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))
MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "800"))
TOP_P = float(os.getenv("LLM_TOP_P", "1.0"))
TIMEOUT = float(os.getenv("LLM_TIMEOUT_SEC", "30"))  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)
CHUNK_SIZE = int(os.getenv("LLM_CHUNK_SIZE", "600"))  # WSë¡œ ë³´ë‚¼ ì¡°ê° í¬ê¸°

# ===== Utils =====
def _chunk_text(s: str, n: int) -> Iterable[str]:
    for i in range(0, len(s), n):
        yield s[i : i + n]


def _log_bad_request(prefix: str, err: BadRequestError) -> None:
    try:
        logger.error("%s | BadRequestError: %s", prefix, str(err))
        resp = getattr(err, "response", None)
        if resp is not None:
            try:
                logger.error("%s | response.status=%s", prefix, resp.status_code)
            except Exception:
                pass
            try:
                logger.error("%s | response.json=%s", prefix, resp.json())
            except Exception:
                try:
                    logger.error("%s | response.text=%s", prefix, resp.text)
                except Exception:
                    pass
    except Exception:
        logger.exception("%s | failed to log BadRequestError", prefix)


def _safe_chat_create(client: OpenAI, *, model: str, messages: list[dict], stream: bool):
    """
    íŒŒë¼ë¯¸í„° í˜¸í™˜ì„± ì´ìŠˆì— ëŒ€ë¹„í•´ ë‹¨ê³„ì ìœ¼ë¡œ ì˜µì…˜ì„ ì¤„ì´ë©° í˜¸ì¶œ.
    1) temperature + top_p + max_completion_tokens
    2) temperature + max_completion_tokens
    3) max_completion_tokens
    4) (ìµœì†Œ) í•„ìˆ˜ ì¸ìë§Œ
    """
    attempts = [
        dict(model=model, messages=messages, stream=stream,
            temperature=TEMPERATURE, top_p=TOP_P, max_tokens=MAX_TOKENS),
        dict(model=model, messages=messages, stream=stream,
            temperature=TEMPERATURE, max_tokens=MAX_TOKENS),
        dict(model=model, messages=messages, stream=stream,
            max_tokens=MAX_TOKENS),
        dict(model=model, messages=messages, stream=stream),
    ]
    last_err: Optional[Exception] = None
    for i, payload in enumerate(attempts, 1):
        try:
            return client.chat.completions.create(**payload)
        except BadRequestError as e:
            _log_bad_request(f"chat.create attempt#{i}", e)
            last_err = e
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ì˜ˆ ë¶ˆê°€(ì¡°ì§ ë¯¸ì¸ì¦/ëª¨ë¸ ë¯¸ì§€ì›)ë©´ ìƒìœ„ì—ì„œ í´ë°±ì‹œí‚¤ë„ë¡ ì¦‰ì‹œ ì¬ë˜ì§
            if stream and "param" in str(e).lower() and "stream" in str(e).lower():
                raise
        except Exception as e:
            logger.exception("chat.create attempt#%d unexpected error", i)
            last_err = e
    if last_err:
        raise last_err


def _build_messages(*, system_prompt: str, recent_steps, user_input: str) -> list[dict]:
    msgs: list[dict] = [{"role": "system", "content": system_prompt}]
    for step in recent_steps:
        if getattr(step, "user_input", None):
            msgs.append({"role": "user", "content": step.user_input})
        if getattr(step, "gpt_response", None):
            msgs.append({"role": "assistant", "content": step.gpt_response})
    msgs.append({"role": "user", "content": user_input})
    return msgs

# ===== Public API =====
async def stream_noa_response(*, user_input, session, recent_steps, system_prompt):
    client = OpenAI(timeout=TIMEOUT)
    messages = _build_messages(system_prompt=system_prompt, recent_steps=recent_steps, user_input=user_input)

    try:
        logger.info("LLM: streaming path selected (attempting stream=True)")
        stream = _safe_chat_create(client, model=MODEL, messages=messages, stream=True)

        yielded = False
        for event in stream:
            piece = _extract_text_from_stream_event(event)
            if piece:
                yielded = True
                yield piece

        if not yielded:
            logger.warning("LLM: stream yielded no content; falling back to non-streaming")
            resp = _safe_chat_create(client, model=MODEL, messages=messages, stream=False)
            text = _extract_text_from_chat_completion(resp).strip()
            if not text:
                # ğŸ‘‰ ì—¬ê¸°ì„œ ë°”ë¡œ ì˜ˆì™¸ë¥¼ ë˜ì§€ì§€ ë§ê³ , 'ë°±ì—… ëª¨ë¸' ì‹œë„
                raise RuntimeError("empty_completion_from_llm")
            for chunk in _chunk_text(text, CHUNK_SIZE):
                yield chunk
        return


    except BadRequestError as e:
        emsg = str(e).lower()
        if "must be verified to stream this model" in emsg or ("'param': 'stream'" in emsg and "unsupported_value" in emsg):
            logger.warning("LLM: stream not allowed; falling back to non-streaming (policy)")
        else:
            raise
    except Exception:
        # âœ… ìˆ˜ì •: ë¹ˆ 'logger' ì°¸ì¡° ëŒ€ì‹  ì œëŒ€ë¡œ ë¡œê·¸ ë‚¨ê¸°ê¸°
        logger.exception("LLM: streaming failed unexpectedly; falling back to non-streaming")

    # (ê¸°ì¡´) í´ë°± ê²½ë¡œ ìœ ì§€
    logger.info("LLM: non-streaming fallback path selected (stream=False)")
    resp = _safe_chat_create(client, model=MODEL, messages=messages, stream=False)
    text = _extract_text_from_chat_completion(resp).strip()
    if not text:
        raise RuntimeError("empty_completion_from_llm")
    for chunk in _chunk_text(text, CHUNK_SIZE):
        yield chunk



def generate_noa_response(*, user_input: str, recent_steps, system_prompt: str) -> str:
    """
    í•˜ìœ„ í˜¸í™˜(ë™ê¸°/ë‹¨ë°œ ì‘ë‹µ). ìµœì‹  íŒŒë¼ë¯¸í„° ê·œì¹™ìœ¼ë¡œ ë‹¨ë°œ ì‘ë‹µì„ ë°˜í™˜.
    """
    client = OpenAI(timeout=TIMEOUT)
    messages = _build_messages(system_prompt=system_prompt, recent_steps=recent_steps, user_input=user_input)
    resp = _safe_chat_create(client, model=MODEL, messages=messages, stream=False)
    try:
        return resp.choices[0].message.content or ""
    except Exception:
        logger.exception("generate_noa_response: failed to extract content")
        return ""

# âœ… ì¶”ê°€: Chat Completions 'ë‹¨ë°œ' ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def _extract_text_from_chat_completion(resp) -> str:
    try:
        choice = getattr(resp, "choices", [None])[0]
        if not choice:
            return ""
        msg = getattr(choice, "message", None)
        finish_reason = getattr(choice, "finish_reason", None)

        # 1) contentê°€ ë¬¸ìì—´
        content = getattr(msg, "content", None)
        if isinstance(content, str) and content.strip():
            return content

        # 2) contentê°€ íŒŒì¸  ë°°ì—´ (ë©€í‹°ëª¨ë‹¬ í…ìŠ¤íŠ¸ íŒŒíŠ¸)
        if isinstance(content, list):
            parts = []
            for p in content:
                if isinstance(p, dict) and p.get("type") == "text":
                    parts.append(p.get("text") or "")
            if "".join(parts).strip():
                return "".join(parts)

        # 3) íˆ´ì½œë§Œ ìˆëŠ” ì¼€ì´ìŠ¤: ìš°ë¦¬ ì„œë¹„ìŠ¤ëŠ” íˆ´ì½œ ë¯¸ì‚¬ìš© â†’ ë¹ˆë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
        tool_calls = getattr(msg, "tool_calls", None) or getattr(msg, "function_call", None)
        if tool_calls:
            return ""  # (ì„ íƒ) "[tool_call]" ê°™ì€ í‘œì‹ì„ ë„£ì–´ë„ ë¨

        # 4) ì•ˆì „í•„í„° ì°¨ë‹¨
        if finish_reason == "content_filter":
            raise RuntimeError("blocked_by_content_filter")

        return ""
    except Exception:
        logger.exception("extract_text: failed; returning empty")
        return ""


# âœ… ì¶”ê°€: Chat Completions 'ìŠ¤íŠ¸ë¦¼' ì´ë²¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def _extract_text_from_stream_event(event) -> str:
    """
    í‘œì¤€: event.choices[0].delta.content
    ì¼ë¶€ SDK/í™˜ê²½: event.delta / event.data ë“± ë³€í˜• ê°€ëŠ¥ â†’ ë°©ì–´ì½”ë“œ
    """
    try:
        # í‘œì¤€ ê²½ë¡œ
        choices = getattr(event, "choices", None)
        if choices:
            delta = getattr(choices[0], "delta", None)
            content = getattr(delta, "content", None)
            if isinstance(content, str):
                return content

        # ë°©ì–´ì  íŒŒì‹± (dictí™” ì‹œë„)
        if hasattr(event, "model_dump_json"):
            import json
            d = json.loads(event.model_dump_json())
        elif hasattr(event, "dict"):
            d = event.dict()
        else:
            d = getattr(event, "__dict__", {}) or {}

        # ë¸íƒ€ í…ìŠ¤íŠ¸ ìœ„ì¹˜ íƒìƒ‰
        for key in ("content", "text", "delta"):
            v = d.get(key)
            if isinstance(v, str):
                return v
            if isinstance(v, dict) and isinstance(v.get("content"), str):
                return v["content"]

        return ""
    except Exception:
        logger.exception("extract_text_from_stream_event: failed")
        return ""
